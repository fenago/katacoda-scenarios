{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stacking on the sonar dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "from math import exp\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacked Generalization\n",
        "Stacked Generalization or stacking is an ensemble technique that uses a new model to learn how to best combine the predictions from two or more\n",
        "models trained on your dataset.\n",
        "\n",
        "Stacked Generalization is an ensemble algorithm where a new model is trained to combine the\n",
        "predictions from two or more models already trained or your dataset. The predictions from the\n",
        "existing models or submodels are combined using a new model, and as such stacking is often\n",
        "referred to as blending, as the predictions from submodels are blended together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonar Dataset\n",
        "In this section, we will apply the Stacking algorithm to the Sonar dataset. The example assumes that a CSV copy of the dataset is in the current \n",
        "working directory with the filename sonar.all-data.csv.\n",
        "\n",
        "The dataset is first loaded, the string values converted to numeric and the output column is converted from strings to the integer values of 0 to 1.\n",
        "This is achieved with helper functions load csv(), str column to float() and str column to int() to load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use k-fold cross-validation to estimate the performance of the learned model on\n",
        "unseen data. This means that we will construct and evaluate k models and estimate the\n",
        "performance as the mean model error. Classification accuracy will be used to evaluate the\n",
        "model. These behaviors are provided in the cross validation split(), accuracy metric()\n",
        "and evaluate algorithm() helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor _ in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submodels and Aggregator\n",
        "We are going to use two models as submodels for stacking and a linear model as the aggregator\n",
        "model.\n",
        "This part is divided into 3 sections:\n",
        "1. Submodel #1: k-Nearest Neighbors.\n",
        "2. Submodel #2: Perceptron.\n",
        "3. Aggregator Model: Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below are the helper functions that involve making predictions for a KNN model. The function euclidean distance() calculates the distance between two rows of data, get neighbors()\n",
        "function locates all neighbors in the training dataset for a new row of data and knn predict()\n",
        "makes a prediction from the neighbors for a new row of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the Euclidean distance between two vectors\n",
        "def euclidean_distance(row1, row2):\n",
        "\tdistance = 0.0\n",
        "\tfor i in range(len(row1)-1):\n",
        "\t\tdistance += (row1[i] - row2[i])**2\n",
        "\treturn sqrt(distance)\n",
        "\n",
        "# Locate neighbors for a new row\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "\tdistances = list()\n",
        "\tfor train_row in train:\n",
        "\t\tdist = euclidean_distance(test_row, train_row)\n",
        "\t\tdistances.append((train_row, dist))\n",
        "\tdistances.sort(key=lambda tup: tup[1])\n",
        "\tneighbors = list()\n",
        "\tfor i in range(num_neighbors):\n",
        "\t\tneighbors.append(distances[i][0])\n",
        "\treturn neighbors\n",
        "\n",
        "# Make a prediction with kNN\n",
        "def knn_predict(model, test_row, num_neighbors=2):\n",
        "\tneighbors = get_neighbors(model, test_row, num_neighbors)\n",
        "\toutput_values = [row[-1] for row in neighbors]\n",
        "\tprediction = max(set(output_values), key=output_values.count)\n",
        "\treturn prediction"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submodel #1: k-Nearest Neighbors\n",
        "The k-Nearest Neighbors algorithm or KNN uses the entire training dataset as the model.\n",
        "Therefore training the model involves retaining the training dataset. Below is a function named\n",
        "knn model() that does just this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare the kNN model\n",
        "def knn_model(train):\n",
        "\treturn train\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submodel #2: Perceptron\n",
        "The model for the Perceptron algorithm is a set of weights learned from the training data. In\n",
        "order to train the weights, many predictions need to be made on the training data in order\n",
        "to calculate error values. Therefore, both model training and prediction require a function for\n",
        "prediction.\n",
        "Below are the helper functions for implementing the Perceptron algorithm. The perceptron model()\n",
        "function trains the Perceptron model on the training dataset and perceptron predict() is\n",
        "used to make a prediction for a row of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make a prediction with weights\n",
        "def perceptron_predict(model, row):\n",
        "\tactivation = model[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += model[i + 1] * row[i]\n",
        "\treturn 1.0 if activation >= 0.0 else 0.0\n",
        "\n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor _ in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = perceptron_predict(weights, row)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\treturn weights\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregator Model: Logistic Regression\n",
        "Like the Perceptron algorithm, Logistic Regression uses a set of weights, called coefficients, as\n",
        "the representation of the model. And like the Perceptron algorithm, the coefficients are learned\n",
        "by iteratively making predictions on the training data and updating them.\n",
        "Below are the helper functions for implementing the logistic regression algorithm. The\n",
        "logistic regression model() function is used to train the coefficients on the training dataset\n",
        "and logistic regression predict() is used to make a prediction for a row of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make a prediction with coefficients\n",
        "def logistic_regression_predict(model, row):\n",
        "\tyhat = model[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tyhat += model[i + 1] * row[i]\n",
        "\treturn 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
        "\tcoef = [0.0 for i in range(len(train[0]))]\n",
        "\tfor _ in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tyhat = logistic_regression_predict(coef, row)\n",
        "\t\t\terror = row[-1] - yhat\n",
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "\treturn coef\n",
        "\n",
        "# Make predictions with sub-models and construct a new stacked row\n",
        "def to_stacked_row(models, predict_list, row):\n",
        "\tstacked_row = list()\n",
        "\tfor i in range(len(models)):\n",
        "\t\tprediction = predict_list[i](models[i], row)\n",
        "\t\tstacked_row.append(prediction)\n",
        "\tstacked_row.append(row[-1])\n",
        "\treturn row[0:len(row)-1] + stacked_row\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below function named stacking() does 4 things:\n",
        "1. It first trains a list of models (KNN and Perceptron).\n",
        "2. It then uses the models to make predictions and create a new stacked dataset.\n",
        "3. It then trains an aggregator model (logistic regression) on the stacked dataset.\n",
        "4. It then uses the submodels and aggregator model to make predictions on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stacked Generalization Algorithm\n",
        "def stacking(train, test):\n",
        "\tmodel_list = [knn_model, perceptron_model]\n",
        "\tpredict_list = [knn_predict, perceptron_predict]\n",
        "\tmodels = list()\n",
        "\tfor i in range(len(model_list)):\n",
        "\t\tmodel = model_list[i](train)\n",
        "\t\tmodels.append(model)\n",
        "\tstacked_dataset = list()\n",
        "\tfor row in train:\n",
        "\t\tstacked_row = to_stacked_row(models, predict_list, row)\n",
        "\t\tstacked_dataset.append(stacked_row)\n",
        "\tstacked_model = logistic_regression_model(stacked_dataset)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tstacked_row = to_stacked_row(models, predict_list, row)\n",
        "\t\tstacked_dataset.append(stacked_row)\n",
        "\t\tprediction = logistic_regression_predict(stacked_model, stacked_row)\n",
        "\t\tprediction = round(prediction)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn predictions\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A k value of 3 was used for cross-validation, giving each fold 208/3 = 69.3  or just under 70\n",
        "records to be evaluated upon each iteration. Running the example prints the scores and mean\n",
        "of the scores for the final configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test stacking on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "n_folds = 3\n",
        "scores = evaluate_algorithm(dataset, stacking, n_folds)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}