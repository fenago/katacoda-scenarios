Remember that the lower the RMSE the better. These results are telling. Firstly, we can see
that all of the algorithms are better than the baseline skill of ZeroR and that the difference is
significant (a little * next to each score). We can also see that there does not appear to be much
benefit across the evaluated algorithms from standardizing or normalizing the data. It does look
like we may see a small improvement from the selecting less features view of the dataset, at
least for IBk. Finally, it looks like the IBk (KNN) may have the lowest error. Letâ€™s investigate
further.

- Click the Select button for the Test base and choose the IBk algorithm as the new
test base
- Click the Perform test button to rerun the analysis

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-25/steps/images/25.2.png)

We can see that it does look there is a difference between IBk and the other algorithms is
significant, except when comparing to the REPTree algorithm and SMOreg Both the IBk and
SMOreg algorithms are nonlinear regression algorithms that can be further tuned, something
we can look at in the next section
