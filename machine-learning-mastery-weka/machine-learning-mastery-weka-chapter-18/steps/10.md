Support Vector Machines were developed for binary classification problems, although extensions
to the technique have been made to support multiclass classification and regression problems.
The adaptation of SVM for regression is called Support Vector Regression or SVR for short.
SVM was developed for numerical input variables, although will automatically convert nominal
values to numerical values. Input data is also normalized before being used.

Unlike SVM that finds a line that best separates the training data into classes, SVR works
by finding a line of best fit that minimizes the error of a cost function. This is done using an
optimization process that only considers those data instances in the training dataset that are
closest to the line with the minimum cost. These instances are called support vectors, hence the
name of the technique. In almost all problems of interest, a line cannot be drawn to best fit the
data, therefore a margin is added around the line to relax the constraint, allowing some bad
predictions to be tolerated but allowing a better result overall.

Finally, few datasets can be fit with just a straight line. Sometimes a line with curves or
even polygonal regions need to be marked out. This is achieved by projecting the data into a
higher dimensional space in order to draw the lines and make predictions. Different kernels can
be used to control the projection and the amount of flexibility. Choose the SVR algorithm:

1. Click the Choose button and select SMOreg under the function group.
2. Click on the name of the algorithm to review the algorithm configuration.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-18/steps/images/97.png)
