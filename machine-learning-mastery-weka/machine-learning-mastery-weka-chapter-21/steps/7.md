
Note that you can edit the configuration for an algorithm added to the experiment by clicking
the Edit selected... button in the Algorithms pane.
- IBk, k=3, distanceFunction=Euclidean
- IBk, k=7, distanceFunction=Euclidean
- IBk, k=1, distanceFunction=Manhattan
- IBk, k=3, distanceFunction=Manhattan
- IBk, k=7, distanceFunction=Manhattan

Euclidean distance is a distance measure that is best used when all of the input attributes
in the dataset have the same scale. Manhattan distance is a measure that is best used when the
input attributes have varying scales, such as in the case of the Pima Indians onset of diabetes
dataset. We would expect that KNN with a Manhattan distance measure would achieve a better
overall score from this experiment. The experiment uses 10-fold cross-validation (the default).
Each configuration will be evaluated on the dataset 10 times (10 runs of 10-fold cross-validation)
with different random number seeds. This will give 10 slightly different results for each evaluated
algorithm configuration, a small population that we can interpret using statistical methods
later.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-21/steps/images/124.png)
