Support Vector Machines were developed for binary classification problems, although extensions
to the technique have been made to support multiclass classification and regression problems.
The algorithm is often referred to as SVM for short. SVM was developed for numerical input
variables, although will automatically convert nominal values to numerical values. Input data is
also normalized before being used. SVM work by finding a line that best separates the data
into the two groups. This is done using an optimization process that only considers those data
instances in the training dataset that are closest to the line that best separates the classes. The
instances are called support vectors, hence the name of the technique.

In almost all problems of interest, a line cannot be drawn to neatly separate the classes,
therefore a margin is added around the line to relax the constraint, allowing some instances to
be misclassified but allowing a better result overall. Finally, few datasets can be separated with
just a straight line. Sometimes a line with curves or even polygonal regions need to be marked
out. This is achieved with SVM by projecting the data into a higher dimensional space in order
to draw the lines and make predictions. Different kernels can be used to control the projection
and the amount of flexibility in separating the classes. Choose the SVM algorithm:

1. Click the Choose button and select SMO under the function group
2. Click on the name of the algorithm to review the algorithm configuration

SMO refers to the specific efficient optimization algorithm used inside the SVM implementation, which stands for Sequential Minimal Optimization.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/89.png)
