The depth of the tree is defined automatically, but a depth can be specified in the maxDepth
parameter. You can also choose to turn off pruning by setting the noPruning parameter to True,
although this may result in worse performance. The minNum parameter defines the minimum
number of instances supported by the tree in a leaf node when constructing the tree from the
training data.

1. Click OK to close the algorithm configuration
2. Click the Start button to run the algorithm on the Ionosphere dataset
You can see that with the default configuration that the decision tree algorithm achieves an
accuracy of 89%

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/84.png)

Another more advanced decision tree algorithm that you can use is the C4.5 algorithm,
called trees.J48 in Weka. You can review a visualization of a decision tree prepared on the
entire training data set by right clicking on the Result list and clicking Visualize Tree.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/85.png)