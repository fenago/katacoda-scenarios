The size of the neighborhood is controlled by the k parameter, called IBk in Weka. For
example, if k is set to 1, then predictions are made using the single most similar training
instance to a given new pattern for which a prediction is requested. Common values for k
are 3, 7, 11 and 21, larger for larger dataset sizes. Weka can automatically discover a good
value for k using cross-validation inside the algorithm by setting the crossValidate parameter to
True. Another important parameter is the distance measure used. This is configured in the
nearestNeighbourSearchAlgorithm which controls the way in which the training data is stored
and searched. The default is a LinearNNSearch. Clicking the name of this search algorithm will
provide another configuration window where you can choose a distanceFunction parameter. By
default, Euclidean distance is used to calculate the distance between instances, which is good
for numerical data with the same scale. Manhattan distance is good to use if your attributes
differ in measures or type.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/87.png)

It is a good idea to try a suite of different k values and distance measures on your problem
and see what works best
1. Click OK to close the algorithm configuration
2. Click the Start button to run the algorithm on the Ionosphere dataset
You can see that with the default configuration that the KNN algorithm achieves an accuracy
of 86%

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/88.png)
