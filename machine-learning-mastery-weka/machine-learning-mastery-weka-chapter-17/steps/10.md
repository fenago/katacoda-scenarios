The k-nearest neighbors algorithm supports both classification and regression. It is also called
KNN for short. It works by storing the entire training dataset and querying it to locate the k
most similar training patterns when making a prediction. As such, there is no model other than
the raw training dataset and the only computation performed is the querying of the training
dataset when a prediction is requested.

It is a simple algorithm, but one that does not assume very much about the problem other
than that the distance between data instances is meaningful in making predictions. As such,
it often achieves very good performance. When making predictions on classification problems,
KNN will take the mode (most common class) of the k most similar instances in the training
dataset. Choose the k-Nearest Neighbors algorithm:

1. Click the Choose button and select IBk under the lazy group
2. Click on the name of the algorithm to review the algorithm configuration

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-17/steps/images/86.png)


