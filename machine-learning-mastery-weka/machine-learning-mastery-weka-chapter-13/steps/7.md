Another popular feature selection technique is to calculate the information gain. You can
calculate the information gain (also called entropy) for each attribute for the output variable.
Entry values vary from 0 (no information) to 1 (maximum information). Those attributes that
contribute more information will have a higher information gain value and can be selected,
whereas those that do not add much information will have a lower score and can be removed.

Weka supports feature selection via information gain using the InfoGainAttributeEval
Attribute Evaluator. Like the correlation technique above, the Ranker Search Method method
must be used. Running this technique on our Pima Indians dataset we can see that one attribute
contributes more information than all of the others (plas). If we use an arbitrary cutoff of 0.05,
then we would also select the mass, age and insu attributes and drop the rest from our dataset.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-13/steps/images/63.png)