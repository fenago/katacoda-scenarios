A popular technique for selecting the most relevant attributes in your dataset is to use correlation.
More formally, correlation can be calculated to determine how much two variables change
together, either in the same or differing directions on the number line. You can calculate the
correlation between each attribute and the output variable and select only those attributes
that have a moderate-to-high positive or negative correlation (close to -1 or 1) and drop those
attributes with a low correlation (value close to zero).

Weka supports correlation based feature selection with the CorrelationAttributeEval technique
that requires use of a Ranker Search Method. Running this on our Pima Indians dataset suggests
that one attribute (plas) has the highest correlation with the output class. It also suggests a
host of attributes with some modest correlation (mass, age, preg). If we use 0.2 as our cut-off
for relevant attributes, then the remaining attributes could possibly be removed (pedi, insu,
skin and pres)

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-13/steps/images/62.png)
