A popular feature selection technique is to use a generic but powerful learning algorithm and
evaluate the performance of the algorithm on the dataset with different subsets of attributes
selected. The subset that results in the best performance is taken as the selected subset. The
algorithm used to evaluate the subsets does not have to be the algorithm that you intend to use
to model your problem, but it should be generally quick to train and powerful, like a decision
tree method. In Weka this type of feature selection is supported by the WrapperSubsetEval
technique and must use a GreedyStepwise or BestFirst Search Method. The latter, BestFirst, is
preferred if you can spare the compute time.

- First select the WrapperSubsetEval technique
- Click on the name WrapperSubsetEval to open the configuration for the method
- Click the Choose button for the classifier parameter and change it to trees.J48

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-13/steps/images/64.png)
