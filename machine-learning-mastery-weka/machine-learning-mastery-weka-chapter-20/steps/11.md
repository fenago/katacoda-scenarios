
We can compare all of the algorithm results to one base result This can be specified by the
Test base option The default is the first algorithm evaluated in the list, which in this case is
Logistic regression We can see this by clicking the Select button next to Test base

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-20/steps/images/117.png)

Click the Perform test button in the Actions pane to perform the statistical test and produce
some output we can review You should see results like those listed below

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-20/steps/images/20.1.png)

We can see that Logistic, our base for comparison marked as (1) has the accuracy of 77.47%
on the problem. This result is compared to the other 4 algorithms and indicated with a number
and mapped in a legend at the bottom below the table of results. Note the * next to the IBk
and REPTree results. This indicates that the results are significantly different from the Logistic
results, but the scores are lower. NaiveBayes and SMO do not have any character next to their
results in the table, indicating that the results are not significantly different from Logistic. If an
algorithm had results larger than the base algorithm and the difference was significant, a little v
would appear next to the results.

If we had to build a model immediately, we might choose Logistic, but we might also choose
NaiveBayes or SMO as their results were not significantly different. Logistic regression is a
good model to choose because it is simple, we understood and fast to train. We would probably
not choose IBk or the decision tree, at least not their default configurations because we know
Logistic can do better and that result is statistically significant.