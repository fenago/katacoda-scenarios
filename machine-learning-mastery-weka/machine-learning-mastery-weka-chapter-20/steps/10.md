The third tab of the Weka Experiment Environment is for analyzing experimental results

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-20/steps/images/115.png)

You can load results from:
- A file, if you configured your experiment to save results to a file on the Setup tab
- A Database, if you configured your experiment to save results to a database on the Setup
tab
- A Experiment, if you just ran an experiment in the Experiment Environment (which we
just did)

Load the results from the experiment we just executed by clicking the Experiment button in
the Source pane You will see that 500 results were loaded This is because we had 5 algorithms
that were each evaluated 100 times, 10-fold cross-validation multiplied by 10 repeats

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-20/steps/images/116.png)

Results were collected for many different performance measures, such as classification
accuracy The Weka Experiment Environment allows us to performs statistical tests on the
different performance measures to allow us to draw conclusions from the experiment For
example, we are interested in two questions from our experiments:
- Which algorithm evaluated in the experiment had the best performance? This is useful to
know if we wanted to create a good performance model immediately
- What is the rank of algorithms by performance? This is useful to know if we want to
further investigate and tune the 2-to-3 algorithms that performed the best on the problem

We can configure the result summary to display in the Configure test pane. The type of
statistical test can be selected in the Testing with option, by default this is set to Paired T-Tester
(corrected). This is just fine and will compare each algorithm in pairs and make some reasonable
assumptions about the distribution of the results collected, such as that they are drawn from a
Gaussian distribution. The significance level is set in the Significance parameter and is default
to 0.05 (5%), which again, is just fine.

We do not need get caught up in the technical details of statistical significance testing. These
useful defaults will inform us whether the differences between any of the pairwise algorithm
performance comparisons we review are statistically significant with a confidence of 95%. We
can choose the performance measure by which to compare the algorithms in the Comparison
field option. The default is the Percent correct metric (accuracy) which is exactly what we are
interested in as a first pass.