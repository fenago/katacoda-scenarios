Note the red and blue colors referring to the positive and negative classes respectively. The
colors are assigned automatically to each categorical value. If there were three categories for
the class value, we would see the breakdown of the preg distribution by three colors rather
than two. This is useful to get a quick idea of whether the problem is easily separable for a
given attribute, e.g. all the red and blue are cleanly separated for a single attribute. Clicking
through each attribute in the list of Attributes and reviewing the plots, we can see that there is
no such easy separation of the classes. We can quickly get an overview of the distribution of all
attributes in the dataset and the breakdown of distributions by class by clicking the Visualize
All button above the univariate plot.

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-09/steps/images/64-31.png)

Looking at these plots we can see a few interesting things about this dataset.
- It looks like the plas, pres and mass attributes have a nearly Gaussian distribution.
- It looks likes pres, skin, insu and mass have values at 0 that look out of place.

Looking at plots like this and jotting down things that come to mind can give you an idea of
further data preparation operations that could be applied (like marking zero values as corrupt)
and even techniques that might be useful (like linear discriminant analysis and logistic regression
that assume a Gaussian distribution in input variables).