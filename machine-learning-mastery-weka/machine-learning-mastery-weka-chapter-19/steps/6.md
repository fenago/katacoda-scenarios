Random Forest is an extension of bagging for decision trees that can be used for classification
or regression. A down side of bagged decision trees is that decision trees are constructed using
a greedy algorithm that selects the best split point at each step in the tree building process. As
such, the resulting trees end up looking very similar which reduces the variance of the predictions
from all the bags which in turn hurts the robustness of the predictions made.

Random Forest is an improvement upon bagged decision trees that disrupts the greedy
splitting algorithm during tree creation so that split points can only be selected from a random
subset of the input attributes. This simple change can have a big effect decreasing the similarity
between the bagged trees and in turn the resulting predictions. Choose the random forest
algorithm:

1. Click the Choose button and select RandomForest under the trees group
2. Click on the name of the algorithm to review the algorithm configuration

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-19/steps/images/103.png)

