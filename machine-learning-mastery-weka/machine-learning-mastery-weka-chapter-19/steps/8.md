AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a
group of ensemble methods called boosting, that add new machine learning models in a series
where subsequent models attempt to fix the prediction errors made by prior models. AdaBoost
was the first successful implementation of this type of model. AdaBoost was designed to use
short decision tree models, each with a single decision point. Such short trees are often referred
to as decision stumps.

The first model is constructed as per normal. Each instance in the training dataset is
weighted and the weights are updated based on the overall accuracy of the model and whether
an instance was classified correctly or not. Subsequent models are trained and added until a
minimum accuracy is achieved or no further improvements are possible. Each model is weighted
based on its skill and these weights are used when combining the predictions from all of the
models on new data. Choose the AdaBoost algorithm:
1. Click the Choose button and select AdaBoostM1 under the meta group
2. Click on the name of the algorithm to review the algorithm configuration

![](https://github.com/fenago/katacoda-scenarios/raw/master/machine-learning-mastery-weka/machine-learning-mastery-weka-chapter-19/steps/images/105.png)
