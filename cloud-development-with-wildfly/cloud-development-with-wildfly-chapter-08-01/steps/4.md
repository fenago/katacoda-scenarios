Load balancing
--------------

* * * * *

We have just learned how to scale our service. The next natural step is
to configure the load balancer. The good news is that OpenShift will do
most of the stuff automatically for us.

In [Chapter
6](https://subscription.packtpub.com/book/web_development/9781786462374/6), *Deploying
Applications on the Cloud with OpenShift*, where we introduced services,
we learned that a service is reached using a virtual cluster IP. To
understand how load balancing works, let's understand how cluster IP is
implemented.

As we have also learned here, each node in a Kubernetes cluster runs a
bunch of services, which allow a cluster to provide its functionality.
One of those services is **kube-proxy**. Kube-proxy runs on every node
and is, among other things, responsible for service implementation.
Kube-proxy continuously monitors the object model describing the cluster
and gathers information about currently active services and pods on
which those services run. When the new service appears, kube-proxy
modifies the iptables rules so that the virtual cluster's IP is routed
to one of the available pods. The iptables rules are created so that the
choice of the pod is random. Also, note that those IP rules have to be
constantly rewritten to match the current state of the cluster.

A kube-proxy runs on every node of the cluster. Owing to that, on each
node, there is a set of iptables rules, which forward the package to the
appropriate pods. As a result, the service is accessible from each node
of the cluster on its virtual cluster IP.

What's the implication of that from the client service perspective? The
cluster infrastructure is hidden from the service client. The client
doesn't need to have any knowledge about nodes, pods, and their dynamic
movement inside the cluster. They just invoke the service using its IP
as if it was a physical host. 

Let's return to our example and look at the load balancing of our host.
Let's return to the example in which we are working within this chapter.
We statically scaled our catalog service to five instances. Let's enter
the web console in order to look at all the pods on which the
application currently runs:

![](https://github.com/athertahir/katacoda-scenarios/raw/master/cloud-development-with-wildfly/cloud-development-with-wildfly-chapter-08-01/images/2a3a4392-f188-41ae-96d8-74e1f35c212f.png)

Let's trace to which pods are the requests forwarded. In order to
achieve that, we implemented a simple REST filter:

```
package org.packt.swarm.petstore.catalog;

import javax.ws.rs.container.ContainerResponseFilter;
import javax.ws.rs.container.ContainerRequestContext;
import javax.ws.rs.container.ContainerResponseContext;
import javax.ws.rs.ext.Provider;
import java.io.IOException;

//1
@Provider
public class PodNameResponseFilter implements ContainerResponseFilter {
public void filter(ContainerRequestContext req, ContainerResponseContext res)
throws IOException
    {
//2
        res.getHeaders().add("pod",System.getenv("HOSTNAME"));
}
}
```

The preceding filter adds a `"pod"` property to the response
headers. The filter will be evaluated after the response is processed
(1). On each pod, there is a `"HOSTNAME"` environment variable
set. We can use this variable and add it to the response metadata (2).

As a result, we are ready to trace the load balancing:

![](https://github.com/athertahir/katacoda-scenarios/raw/master/cloud-development-with-wildfly/cloud-development-with-wildfly-chapter-08-01/images/e4f08023-82dc-4337-af5f-0a7e9e67a789.png)

In the preceding screenshot, note that the request is being
automatically load balanced among the available pods.
