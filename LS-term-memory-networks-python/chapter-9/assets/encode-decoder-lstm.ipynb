{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "# generate lists of random integers and their sum\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "\tX, y = list(), list()\n",
    "\tfor _ in range(n_examples):\n",
    "\t\tin_pattern = [randint(1,largest) for _ in range(n_numbers)]\n",
    "\t\tout_pattern = sum(in_pattern)\n",
    "\t\tX.append(in_pattern)\n",
    "\t\ty.append(out_pattern)\n",
    "\treturn X, y\n",
    "\n",
    "# convert data to strings\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "\tmax_length = int(n_numbers * ceil(log10(largest+1)) + n_numbers - 1)\n",
    "\tXstr = list()\n",
    "\tfor pattern in X:\n",
    "\t\tstrp = '+'.join([str(n) for n in pattern])\n",
    "\t\tstrp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "\t\tXstr.append(strp)\n",
    "\tmax_length = int(ceil(log10(n_numbers * (largest+1))))\n",
    "\tystr = list()\n",
    "\tfor pattern in y:\n",
    "\t\tstrp = str(pattern)\n",
    "\t\tstrp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "\t\tystr.append(strp)\n",
    "\treturn Xstr, ystr\n",
    "\n",
    "# integer encode strings\n",
    "def integer_encode(X, y, alphabet):\n",
    "\tchar_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\tXenc = list()\n",
    "\tfor pattern in X:\n",
    "\t\tinteger_encoded = [char_to_int[char] for char in pattern]\n",
    "\t\tXenc.append(integer_encoded)\n",
    "\tyenc = list()\n",
    "\tfor pattern in y:\n",
    "\t\tinteger_encoded = [char_to_int[char] for char in pattern]\n",
    "\t\tyenc.append(integer_encoded)\n",
    "\treturn Xenc, yenc\n",
    "\n",
    "# one hot encode\n",
    "def one_hot_encode(X, y, max_int):\n",
    "\tXenc = list()\n",
    "\tfor seq in X:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(max_int)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tXenc.append(pattern)\n",
    "\tyenc = list()\n",
    "\tfor seq in y:\n",
    "\t\tpattern = list()\n",
    "\t\tfor index in seq:\n",
    "\t\t\tvector = [0 for _ in range(max_int)]\n",
    "\t\t\tvector[index] = 1\n",
    "\t\t\tpattern.append(vector)\n",
    "\t\tyenc.append(pattern)\n",
    "\treturn Xenc, yenc\n",
    "\n",
    "# generate an encoded dataset\n",
    "def generate_data(n_samples, n_numbers, largest, alphabet):\n",
    "\t# generate pairs\n",
    "\tX, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "\t# convert to strings\n",
    "\tX, y = to_string(X, y, n_numbers, largest)\n",
    "\t# integer encode\n",
    "\tX, y = integer_encode(X, y, alphabet)\n",
    "\t# one hot encode\n",
    "\tX, y = one_hot_encode(X, y, len(alphabet))\n",
    "\t# return as numpy arrays\n",
    "\tX, y = array(X), array(y)\n",
    "\treturn X, y\n",
    "\n",
    "# invert encoding\n",
    "def invert(seq, alphabet):\n",
    "\tint_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\tstrings = list()\n",
    "\tfor pattern in seq:\n",
    "\t\tstring = int_to_char[argmax(pattern)]\n",
    "\t\tstrings.append(string)\n",
    "\treturn ''.join(strings)\n",
    "\n",
    "# configure problem\n",
    "\n",
    "# number of math terms\n",
    "n_terms = 2\n",
    "# largest value for any single input digit\n",
    "largest = 5\n",
    "# scope of possible symbols for each input or output time step\n",
    "alphabet = [str(x) for x in range(10)] + ['+', ' ']\n",
    "\n",
    "# size of alphabet: (12 for 0-9, + and ' ')\n",
    "n_chars = len(alphabet)\n",
    "# length of encoded input sequence (8 for '10+10+10)\n",
    "n_in_seq_length = int(n_terms * ceil(log10(largest+1)) + n_terms - 1)\n",
    "# length of encoded output sequence (2 for '30')\n",
    "n_out_seq_length = int(ceil(log10(n_terms * (largest+1))))\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(n_in_seq_length, n_chars)))\n",
    "model.add(RepeatVector(n_out_seq_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# fit LSTM\n",
    "X, y = generate_data(750, n_terms, largest, alphabet)\n",
    "model.fit(X, y, epochs=2, batch_size=32)\n",
    "\n",
    "# evaluate LSTM\n",
    "X, y = generate_data(10, n_terms, largest, alphabet)\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "print('Loss: %f, Accuracy: %f' % (loss, acc*100))\n",
    "\n",
    "# predict\n",
    "for _ in range(5):\n",
    "\t# generate an input-output pair\n",
    "\tX, y = generate_data(1, n_terms, largest, alphabet)\n",
    "\t# make prediction\n",
    "\tyhat = model.predict(X, verbose=0)\n",
    "\t# decode input, expected and predicted\n",
    "\tin_seq = invert(X[0], alphabet)\n",
    "\tout_seq = invert(y[0], alphabet)\n",
    "\tpredicted = invert(yhat[0], alphabet)\n",
    "\tprint('%s = %s (expect %s)' % (in_seq, predicted, out_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
